{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67d0ede",
   "metadata": {},
   "source": [
    "# reduce the dataset and count words\n",
    "\n",
    "- After splitting the lines in the file into a long list of words using `flatMap()` transformation, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. \n",
    "- you'll next create a pair RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, pair RDD is composed of `(w, 1)` where `w` is for each word in the RDD and `1` is a number. Finally, you'll combine the values with the same key from the pair RDD using `reduceByKey()` operation\n",
    "\n",
    " - Remember you already have a `SparkContext` `sc` and `splitRDD` available in your workspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344e9ec",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Please perform following activities\n",
    "\n",
    "    1) Load the data from visits.txt on HDFS into a Rdd\n",
    "    2) Count the number of elements in the above Rdd\n",
    "    3) Verify this count against the number of lines in the file\n",
    "    4) Select only those records from Rdd where the 19th field matches POTUS and then \n",
    "        a) Capture the 0th field  (This datapoint is lname in the data set)\n",
    "        b) Capture the 1st field (This datapoint is fname in the data set)\n",
    "        c) Capture the 6th field (This datapoint is time_of_arrival in the data set)\n",
    "        d) Capture the 11th field (This datapoint is appt_scheduled_time in the data set)\n",
    "        e) Capture the 21st field (This datapoint is location in the data set)\n",
    "        f) Capture the 25th field (This datapoint is comment in the data set)\n",
    "    5) Count the number of records in a final Rdd\n",
    "\n",
    "    • Dataset\n",
    "        ◦ dataset/visits.txt\n",
    "    • TimeLine\n",
    "        ◦ 45 Mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115c6eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 2),\n",
       " ('the', 662),\n",
       " ('People', 2),\n",
       " ('of', 493),\n",
       " ('United', 85),\n",
       " ('States,', 55),\n",
       " ('in', 137),\n",
       " ('Order', 1),\n",
       " ('to', 183),\n",
       " ('form', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc \\\n",
    ".textFile(\"file:///home/talentum/test-jupyter/P2/M1/sm3/day3/p2/Dataset/constitution.txt\") \\\n",
    ".flatMap(lambda line : line.split(\" \")) \\\n",
    ".map(lambda w: (w, 1)) \\\n",
    ".reduceByKey(lambda x, y: x + y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75addae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447598\n",
      "447598\n"
     ]
    }
   ],
   "source": [
    "# File path for the \"visit\" dataset\n",
    "file_path = \"file:///home/talentum/test-jupyter/P2/M1/sm3/day3/Eval_Labs/dataset/visits.txt\"\n",
    "\n",
    "# Creating the base RDD\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "print(baseRDD.count())\n",
    "\n",
    "splitRDD = baseRDD.flatMap(lambda line : line.split('\\n'))\n",
    "\n",
    "print(splitRDD.count())\n",
    "#splitRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "165d95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_RDD = splitRDD.map(lambda x : x.split(\",\"))\n",
    "\n",
    "split_RDD_potus = split_RDD.filter(lambda x : x[19] == \"POTUS\")\n",
    "\n",
    "split_RDD_potus_filter = split_RDD_potus.map(lambda x : (x[0],x[1], x[6], x[11], x[21], x[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "265371c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21819"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_RDD_potus_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73d4d69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BUCKLEY', 'SUMMER', '10/12/2010 14:48', '10/12/2010 14:45', 'WH', ''),\n",
       " ('CLOONEY', 'GEORGE', '10/12/2010 14:47', '10/12/2010 14:45', 'WH', ''),\n",
       " ('PRENDERGAST', 'JOHN', '10/12/2010 14:48', '10/12/2010 14:45', 'WH', '')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_RDD_potus_filter.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2cd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
